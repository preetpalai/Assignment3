{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c1560a",
   "metadata": {},
   "source": [
    "# CSCN8020 â€“ Assignment 3: DQN for Pong\n",
    "\n",
    "> **Note:** This notebook is a **template scaffold**.  \n",
    "> You must complete and adapt it yourself before submitting it as your own work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3196ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6f7d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "# Install missing package(s) in the notebook environment\n",
    "%pip install \"gym[atari]\" -q\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from assignment3_utils import process_frame, transform_reward\n",
    "\n",
    "ENV_NAME = \"PongDeterministic-v4\"\n",
    "\n",
    "IMAGE_SHAPE = (84, 80)\n",
    "STACK_SIZE = 4\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR = 1e-4\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 100_000\n",
    "MIN_REPLAY_SIZE = 10_000\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "TARGET_UPDATE_EVERY = 10\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY_FRAMES = 1_000_000\n",
    "\n",
    "NUM_EPISODES = 300\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.stack(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.stack(next_states),\n",
    "            np.array(dones, dtype=np.uint8),\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274001ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, c, h, w)\n",
    "            n_flatten = self.features(dummy).view(1, -1).size(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8639af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(frame_idx: int) -> float:\n",
    "    if frame_idx >= EPS_DECAY_FRAMES:\n",
    "        return EPS_END\n",
    "    return EPS_START + (EPS_END - EPS_START) * (frame_idx / EPS_DECAY_FRAMES)\n",
    "\n",
    "\n",
    "def select_action(q_net: nn.Module, state_tensor: torch.Tensor, epsilon: float, num_actions: int) -> int:\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(num_actions)\n",
    "    with torch.no_grad():\n",
    "        q_values = q_net(state_tensor)\n",
    "        return int(q_values.argmax(dim=1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29272873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state(env, image_shape, stack_size):\n",
    "    obs = env.reset()\n",
    "    \n",
    "    frame = process_frame(obs, image_shape)\n",
    "    frame = np.squeeze(frame, axis=0)\n",
    "    frame = np.transpose(frame, (2, 0, 1))\n",
    "    \n",
    "    frames = deque([frame] * stack_size, maxlen=stack_size)\n",
    "    state = np.concatenate(frames, axis=0)\n",
    "    return state, frames\n",
    "\n",
    "\n",
    "def update_state(frames, obs, image_shape):\n",
    "    frame = process_frame(obs, image_shape)\n",
    "    frame = np.squeeze(frame, axis=0)\n",
    "    frame = np.transpose(frame, (2, 0, 1))\n",
    "    frames.append(frame)\n",
    "    state = np.concatenate(frames, axis=0)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "num_actions = env.action_space.n\n",
    "print(\"Number of actions:\", num_actions)\n",
    "\n",
    "q_net = DQN((STACK_SIZE, *IMAGE_SHAPE), num_actions).to(device)\n",
    "target_net = DQN((STACK_SIZE, *IMAGE_SHAPE), num_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=LR)\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "print(q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, frames = init_state(env, IMAGE_SHAPE, STACK_SIZE)\n",
    "frame_idx = 0\n",
    "\n",
    "print(\"Warming up replay buffer...\")\n",
    "\n",
    "while len(replay_buffer) < MIN_REPLAY_SIZE:\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    reward = transform_reward(reward)\n",
    "    \n",
    "    next_state = update_state(frames, next_obs, IMAGE_SHAPE)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    frame_idx += 1\n",
    "    \n",
    "    if done:\n",
    "        state, frames = init_state(env, IMAGE_SHAPE, STACK_SIZE)\n",
    "\n",
    "print(\"Replay buffer size:\", len(replay_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "moving_avg_rewards = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state, frames = init_state(env, IMAGE_SHAPE, STACK_SIZE)\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    while not done:\n",
    "        frame_idx += 1\n",
    "        epsilon = get_epsilon(frame_idx)\n",
    "        \n",
    "        state_tensor = torch.from_numpy(state).unsqueeze(0).float().to(device)\n",
    "        \n",
    "        action = select_action(q_net, state_tensor, epsilon, num_actions)\n",
    "        \n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        reward = transform_reward(reward)\n",
    "        \n",
    "        next_state = update_state(frames, next_obs, IMAGE_SHAPE)\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "        \n",
    "        states_t = torch.from_numpy(states).float().to(device)\n",
    "        actions_t = torch.from_numpy(actions).long().unsqueeze(1).to(device)\n",
    "        rewards_t = torch.from_numpy(rewards).to(device)\n",
    "        next_states_t = torch.from_numpy(next_states).float().to(device)\n",
    "        dones_t = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        q_values = q_net(states_t).gather(1, actions_t).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_net(next_states_t).max(1)[0]\n",
    "            targets = rewards_t + GAMMA * next_q_values * (1 - dones_t)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    if len(episode_rewards) >= 5:\n",
    "        moving_avg = np.mean(episode_rewards[-5:])\n",
    "    else:\n",
    "        moving_avg = np.mean(episode_rewards)\n",
    "    moving_avg_rewards.append(moving_avg)\n",
    "    \n",
    "    if (episode + 1) % TARGET_UPDATE_EVERY == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "    \n",
    "    print(f\"Episode {episode+1}/{NUM_EPISODES} | Reward: {episode_reward:.1f} | \"\n",
    "          f\"Avg (last 5): {moving_avg:.2f} | Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e41c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(1, len(episode_rewards) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episodes, episode_rewards, label=\"Reward per episode\")\n",
    "plt.plot(episodes, moving_avg_rewards, label=\"Moving avg (last 5)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(f\"Pong DQN (batch={BATCH_SIZE}, target_update={TARGET_UPDATE_EVERY})\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
